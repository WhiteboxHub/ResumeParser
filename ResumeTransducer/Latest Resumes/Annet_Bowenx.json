{"skills":[{"Technical Skills":":\n\nHadoop\/Big Data Technologies\n\nHDFS, MapReduce, Spark, Hive, Pig, Sqoop, Flume, HBase, Cassandra, Oozie, Zookeeper, YARN, Hue, Ambari, Kafka, Elastic Search, Cloudera, Hortonworks, Tez, Apache Parquet\n\n"},{"Programming Languages":"\n\nJava JDK1.4\/1.5\/1.6 (JDK 5\/JDK 6), C\/C++, Unix Shell Scripting, Python\n\nWeb \/Applications Servers\n\nApache Tomcat, Weblogic, WebSphere and Bastion.\n\nOperating Systems\n\nUNIX, Windows, LINUX\n\nDatabases\n\nHBase, Oracle 8i\/9i\/10g, "}],"work":[{"summary":", DB2 & MySQL 4.x\/5.x, Teradata, Mongo DB\n\nJava IDE\n\nEclipse 3.x, IBM Web Sphere Application Developer, IBM RAD 7.0\n\nTools\n\nTOAD, SQL Developer, SOAP UI, ANT, Maven\n\n","startDate":"2008\/2012","company":"Microsoft"},{},{"summary":" \u2013 Present\n\nSr. Big Data\/Hadoop Developer\n\nResponsibilities:\n\nCoordinated with business customers to gather business requirements. And also interact with other technical peers to derive Technical requirements and delivered the BRD and TDD documents.\n\nExtensively involved in Design phase and delivered Design documents. ","startDate":"Oct 14","company":"American Express"},{},{"summary":"\n\nResponsibilities:\n\nHands on experience in loading data from UNIX file system and Teradata to HDFS\n\nExperienced on loading and transforming of large sets of structured, semi structured and unstructured data from HBase through Sqoop and placed in HDFS for further processing.\n\nInstalled and configured Flume, Hive, Pig, Sqoop and Oozie on the Hadoop cluster.\n\nInvolved in creating Hive tables, loading data and running hive queries in those data.\n\nExtensive Working knowledge of partitioned table, UDFs, performance tuning, compression-related properties, thrift server in Hive.\n\nInvolved in writing optimized Pig Script along with involved in developing and testing Pig Latin Scripts\n\nWorking knowledge in writing Pig\u2019s Load and Store functions.\n\nDeveloped Java MapReduce programs on log data to transform into structured way to find user location, age group, spending time.\n\nDeveloped optimal strategies for distributing the web log data over the cluster, importing and exporting the stored web log data into HDFS and Hive using Scoop.\n\nCollected and aggregated large amounts of web log data from different sources such as webservers, mobile and network devices using Apache Flume and stored the data into HDFS for analysis\n\nMonitored multiple Hadoop clusters environments using Ganglia.\n\nDeveloped PIG scripts for the analysis of semi structured data.\n\nDeveloped and involved in the industry specific UDF (user defined functions).\n\nUsed Flume to collect, aggregate, and store the web log data from different sources like web servers, mobile and network devices and pushed to HDFS.\n\nAnalyzed the web log data using the HiveQL to extract number of unique visitors per day, page views, visit duration, most purchased product on website.\n\nIntegrated Oozie with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box (such as Map-Reduce, Pig, Hive, and Sqoop) as well as system specific jobs (such as Java programs and shell scripts).\n\nMonitored workload, job performance and capacity planning using Cloudera Manager.\n\nManaging and scheduling Jobs on a Hadoop cluster using Oozie.\n\nEnvironment: Amazon EC2, Apache Hadoop 1.0.1, MapReduce, HDFS, CentOS 6.4, HBase, Kafka, Scala, Elastic Search, Hive, Pig, Oozie, Flume, Java (jdk 1.6), Eclipse, Sqoop, Ganglia, Hbase.\n\nKrogers Stores, ","position":"Developer","startDate":"Mar 12","company":"Arlington","endDate":"Sep14"},{"summary":"\n\nResponsibilities:\n\nResponsible for requirement gathering and analysis through interaction with end users.\n\nInvolved in designing use-case diagrams, class diagram, interaction using UML model with Rational Rose.\n\nDesigned and developed the application using various design patterns, such as session facade, business delegate and service locator.Developed UDFs inJava마s and when necessary to use in PIG and HIVE queries.\n\nWorked on Maven build tool.Involved in developing JSP pages using Struts custom tags, JQuery and Tiles Framework.\n\nUsed JavaScript to perform client side validations and Struts-Validator Framework for server-side validation.Developed Web applications with Rich Internet applications using Java applets, Silverlight, JavaFX.\n\nInvolved in creating Database SQL and PL\/SQL queries and stored Procedures.Implemented Singleton classes for property loading and static data from DB and Debugged and developed applications using Rational Application Developer (RAD).\n\nDeveloped a Web service to communicate with the database using SOAP.Developed DAO (data access objects) using Spring Framework 3 and Deployed the components in to WebSphere Application server 7.Actively involved in backend tuning SQL queries\/DB script.Worked in writing commands using UNIX, Shell scripting.\n\nInvolved in developing other subsystems\u2019 server-side components.Production supporting using IBM clear quest for fixing bugs.\n\nGenerated Java wrappers for web services using Apache AXIS\n\nEnvironment : JBoss, XMLSOAP, RESTful, Java EE 6, IBM WebSphere Application Server 7, Apache-Struts 2.0, EJB 3, Spring 3.2, JSP 2.0, Web Services, JQuery 1.7, Servlet 3.0, Struts-Validator, Struts-Tiles, Tag Libraries, ANT 1.5, JDBC, JMS, Service Bus.\n\nGAP Inc,Montgomery, AL Mar08 \u2013 ","position":"Developer","startDate":"Oct 10","company":"Marietta","endDate":"Feb 12"},{"summary":"\n\nResponsibilities:\n\nInvolved in Design, Development and Support phases of Software Development Life Cycle (SDLC). Used agile methodology and participated in Scrum meetings.\n\nDeveloped the application using Spring Framework that leverages Model View Layer (MVC) architecture UML diagrams like use cases, class diagrams, interaction diagrams (sequence and collaboration) and activity diagrams were used. Developed UDFs inJava마s and when necessary.\n\nData from UI layer sent throughJMS to Middle layer and from there using MDB message retrieves Messages and will be sent to MQSeries.\n\nUsed JSON as response type in REST services.\n\nUsed RESTFUL client to interact with the services by providing the RESTFUL URL mapping.\n\nPerformed unit testing using JUnit.Developed UDFs inJava마s and when necessary to use in PIG and HIVE queries.\n\nProvided direction and support for technical architecture with respect to performance, business continuity and seamless integration\/functioning of applications, databases, servers, networks.\n\nManaged, administered and maintain more than 100 Oracle databases.\n\nEnvironment: Java SE 6, Servlets, XML, HTML, JavaScript, JSP, Hibernate, Oracle 11g, SQL Navigator.\n","position":"Developer","startDate":"Sep10"},{}],"basics":{"summary":":\n\nOver8years of professional IT experience with 5+ plus years of experience in analysis, architectural design, prototyping, development, Integration and testing of applications using Java\/J2EE Technologies and 3+ years of experience in Big Data Analytics as Hadoop Developer with good knowledge in Hadoop ecosystem technologies.\n\nDelivery experience on major Hadoop ecosystem Components such as Pig, Hive, Spark Kafka, Elastic Search &Hbase and monitoring with Cloudera Manager.Extensive working experience using Sqoop to import data into HDFS from RDBMS and vice-versa.\n\nIn-depth experience and knowledge in developing and analyzing Map Reduce Jobs and Applications developed standalone and\/or through Pig\/Hive.\n\nExtensive experience in developing Pig Latin Scripts for transformations and using Hive Query Language for data analytics.In depth knowledge of Spark concepts and experience with Spark in Data Transformation and Processing.\n\nHands on experience working on NoSQL databases including Hbase, Cassandra and its integration with Hadoop cluster.Experience in development and utilization of Apache SOLR with Data Computations and Transformation for use by Down Stream Online Applications.\n\nGood experience in ETL tool Informatica.\n\nSolid experience in developing job workflows and schedules with Oozie, and IBM Tivoli\n\n","email":["annebowen@fairhint.com"],"name":Anne Bowen ,"label":"Developer"},"interests":[],"education":[{"Training":" and Knowledge in Mahout, Spark MLlib for use in data classification, regression analysis, recommendation engines and anomaly detection.\n\nGood experience in Python.Developed UDFs inJava마s and when necessary to use in PIG and HIVE queries.\n\nWorking knowledge of database such as Oracle 8i\/9i\/10g, Microsoft SQL Server, DB2, Netezza.\n\nGood experience in Oracle Business Intelligence Enterprise Edition(OBIEE)\n\nExperienced in using Version Control Tools like SubVersion, Git.Experience in development of logging standards and mechanism based on Log4J.Good understanding and experience with Software Development methodologies like Agile and Waterfall.\n\nExperienced in design, development, Unit testing,integration, debugging and implementation and production support, client interaction and understanding business application, business data flow and data relations from them.\n\n"},{"Training":" and Knowledge in Mahout, Spark MLlib for use in data classification, regression analysis, recommendation engines and anomaly detection.Automating the jobs using Unix shell scripting and providing production support.\n\n"}]}